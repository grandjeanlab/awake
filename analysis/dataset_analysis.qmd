---
title: "analysis of the datasets"
author: "Joanes Grandjean"
date: "23/8/2025"
---

specificity analysis of the rs scans. 

first define the plotting engine
```{python}
%autoindent 
import matplotlib
matplotlib.use('TkAgg') 
```


then imort libraries and set functions that we will use later
```{python}

import pandas as pd
from os import listdir
from os.path import join, isfile, isdir

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#write a function get_frame_mask that takes a path, scan_dir, and returns a mask
def dropped_frames(path, scan_dir):
    try:
        frame_mask_path = join(path, scan_dir)
        frame_mask_file = listdir(frame_mask_path)[0]
        frame_mask_full = join(frame_mask_path, frame_mask_file)
        frame_mask_ts = pd.read_table(join(frame_mask_path, frame_mask_file), names=['mask'], skiprows=[0])
        dropped_frames = list(frame_mask_ts["mask"]).count(False)
        return dropped_frames
    except: 
        pass

def total_frames(path, scan_dir):
    try:
        frame_mask_path = join(path, scan_dir)
        frame_mask_file = listdir(frame_mask_path)[0]
        frame_mask_full = join(frame_mask_path, frame_mask_file)
        frame_mask_ts = pd.read_table(join(frame_mask_path, frame_mask_file), names=['mask'], skiprows=[0])
        total_frames =len(list(frame_mask_ts["mask"]))
        return total_frames
    except: 
        pass


#write a function get_seed_ts that takes a path, scan_dir, and returns ts
def get_seed_ts(path, scan_dir, seed):
    seed_prefix = "_seed_name_"
    try:
        seed_path = join(path, scan_dir, seed_prefix + seed)
        seed_file = listdir(seed_path)[0]
        seed_ts = pd.read_table(join(seed_path, seed_file), names=['ts'])
        return seed_ts
    except:
        pass

# correlate the time series of two seeds
def corr_seed(seed1, seed2):
  try:
    return seed1["ts"].corr(seed2["ts"])
  except: 
    pass

# determine the correlation of a reference seed with specific, unspecific, and thalamus seeds
def specific_FC(specific_roi, unspecific_ROI):
  try:
    if (specific_roi>=0.1) and (unspecific_ROI<0.1):
        cat='Specific'
    elif (specific_roi>=0.1) and (unspecific_ROI>=0.1):
        cat='Unspecific'
    elif (abs(specific_roi)<0.1) and (abs(unspecific_ROI)<0.1):
        cat='No'
    else:
        cat='Spurious'
    return cat
  except:
    pass

def get_seed_path(path, scan_dir, seed): 
    seed_prefix = "_seed_name_"
    try:
        seed_path = join(path, scan_dir, seed_prefix + seed)
        seed_file = listdir(seed_path)[0]
        seed_final = join(seed_path, seed_file)
        return seed_final
    except:
        pass


```

now defile some general purpose variables
```{python}

data_dir = "/project/4180000.36/awake/complete_output_mouse"
analysis_dir = "/project/4180000.36/awake/analysis_mouse"
bg_img = "../assets/template/mouse/template.nii.gz"
mask_img = "../assets/template/mouse/mask.nii.gz"

analysis_list = [ "gsr1", "gsr2", "gsr3","wmcsf1", "wmcsf2", "wmcsf3", "aCompCor1", "aCompCor2", "aCompCor3" ]  #to make a loop later on
frame_mask_dir = "frame_censoring_mask"
seed_ts_dir = "analysis_datasink/seed_timecourse_csv/"
seed_img_dir = "analysis_datasink/seed_correlation_maps/"

cat_index = ["Specific", "Non-specific", "Spurious", "No"]

seed_ref = "s1_r"
seed_specific = "s1_l"
seed_unspecific = "aca_r"
seed_thalamus = "vpm_r"

df = pd.read_csv("../assets/tables/mouse_metadata.tsv", sep="\t")
df = df[df['exclude'] != 'y']

df["scan"] = "sub-0" + df["rodent.sub"].astype("str") + "_ses-" + df["rodent.session"].astype("str") + "_run-" + df["rodent.run"].astype("str")
df["scan_dir"] = "_split_name_" + df["scan"] + "_task-rest_bold" 

```

now we run the analysis per denoising style, we extract the number of dropped frames, the s1-s1, s1-aca, and s1-thal correlations. finally we estimate connectivity specificity
```{python}

for analysis in analysis_list:
    print("Running analysis for: " + analysis)
    #get the number of dropped frames for each scan and analysis
    p = join(data_dir, analysis, frame_mask_dir)
    df["dropped.frames." + analysis] = df["scan_dir"].apply(lambda x: dropped_frames(p,x))
    #get the connectivity specificity for s1 and thalamus
    p = join(data_dir, analysis, seed_ts_dir)
    df["s1.specific."+analysis]=df["scan_dir"].apply(lambda x: corr_seed(get_seed_ts(p, x, seed_ref), get_seed_ts(p, x, seed_specific)))
    df["s1.unspecific."+analysis]=df["scan_dir"].apply(lambda x: corr_seed(get_seed_ts(p, x, seed_ref), get_seed_ts(p, x, seed_unspecific)))
    df["thal.specific."+analysis]=df["scan_dir"].apply(lambda x: corr_seed(get_seed_ts(p, x, seed_ref), get_seed_ts(p, x, seed_thalamus)))
    df["s1.cat."+analysis] = df.apply(lambda x: specific_FC(x["s1.specific."+analysis], x["s1.unspecific."+analysis]), axis=1)
    df["thal.cat."+analysis] = df.apply(lambda x: specific_FC(x["thal.specific."+analysis], x["s1.unspecific."+analysis]), axis=1)

df["total.frames"] = df["scan_dir"].apply(lambda x: total_frames(join(data_dir, "gsr3", frame_mask_dir), x))
``` 

get the number of dropped frames for each scan and denoising method, we find that method 1 is leads to an avearge of 67 dropped frames (21% of frames on avg). method 2 leads to 49 dropped frames (16%) and method 3 to 2 dropped frames (0.6%). 
This corresponds to the following rabies flags: 
1: --frame_censoring FD_censoring=true,FD_threshold=0.1,DVARS_censoring=true
2: --frame_censoring FD_censoring=true,FD_threshold=0.5,DVARS_censoring=true
3: --frame_censoring FD_censoring=true,FD_threshold=0.5,DVARS_censoring=false

```{python}

for analysis in analysis_list[0:3]:
    print("Denoising method: " + analysis)
    print("Mean number of dropped frames "+str(df["dropped.frames." + analysis].mean()))
    print("Max number of dropped frames "+str(df["dropped.frames." + analysis].max()))
    print("Mean relative number of dropped frames "+str(df["dropped.frames." + analysis].mean()/df["total.frames"].mean()))
    print("Number of scans with dropped frames: " + str(len(df[df["dropped.frames." + analysis] > 0])))


```

now let's get the average number of dropped frames by datasets. 
```{python}

print("dropped frames using method 1")
for i in np.unique(df["rodent.ds"]):
  print("total and relative dropped frames for dataset "+str(i))
  print(df[df["rodent.ds"]==i]["dropped.frames.gsr1"].mean())
  print(df[df["rodent.ds"]==i]["dropped.frames.gsr1"].mean()/df[df["rodent.ds"]==i]["total.frames"].mean())


print("dropped frames using method 3")
for i in np.unique(df["rodent.ds"]):
  print("total and relative dropped frames for dataset "+str(i))
  print(df[df["rodent.ds"]==i]["dropped.frames.gsr3"].mean())
  print(df[df["rodent.ds"]==i]["dropped.frames.gsr3"].mean()/df[df["rodent.ds"]==i]["total.frames"].mean())

```


get the s1-s1 specificity per dataset for aCompCor1
```{python}

for i in np.unique(df["rodent.ds"]):
  print("dataset "+str(i))
  print(df[df["rodent.ds"]==i]["s1.cat.aCompCor1"].value_counts(normalize=True).reindex(cat_index, fill_value=0))

```
get the s1-s1 specificity per dataset for aCompCor3
```{python}

for i in np.unique(df["rodent.ds"]):
  print("dataset "+str(i))
  print(df[df["rodent.ds"]==i]["s1.cat.aCompCor3"].value_counts(normalize=True).reindex(cat_index, fill_value=0))

```
do the same with gsr1
```{python}

for i in np.unique(df["rodent.ds"]):
  print("dataset "+str(i))
  print(df[df["rodent.ds"]==i]["s1.cat.gsr1"].value_counts(normalize=True).reindex(cat_index, fill_value=0))

```

do the same with gsr3 
```{python}

for i in np.unique(df["rodent.ds"]):
  print("dataset "+str(i))
  print(df[df["rodent.ds"]==i]["s1.cat.gsr3"].value_counts(normalize=True).reindex(cat_index, fill_value=0))

```

plot the s1-s1 specificity for each denoising method

```{python}


fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,8))

sns.set_palette("colorblind")

ax1 = df["s1.cat.gsr1"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 gsr1',ax=axes[0,0], autopct="%.1f%%")
ax2 = df["s1.cat.gsr2"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 gsr2',ax=axes[0,1], autopct="%.1f%%")
ax3 = df["s1.cat.gsr3"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 gsr3',ax=axes[0,2], autopct="%.1f%%")
ax4 = df["s1.cat.wmcsf1"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 wmcsf1',ax=axes[1,0], autopct="%.1f%%")
ax5 = df["s1.cat.wmcsf2"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 wmcsf2',ax=axes[1,1], autopct="%.1f%%")
ax6 = df["s1.cat.wmcsf3"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 wmcsf3',ax=axes[1,2], autopct="%.1f%%")
ax7 = df["s1.cat.aCompCor1"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 aCompCor1',ax=axes[2,0], autopct="%.1f%%")
ax8 = df["s1.cat.aCompCor2"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 aCompCor2',ax=axes[2,1], autopct="%.1f%%")
ax9 = df["s1.cat.aCompCor3"].value_counts(sort=False).reindex(cat_index, fill_value=0).plot.pie(title='s1 aCompCor3',ax=axes[2,2], autopct="%.1f%%")

ax1.set(xlabel="", ylabel="")
ax2.set(xlabel="", ylabel="")
ax3.set(xlabel="", ylabel="")
ax4.set(xlabel="", ylabel="")
ax5.set(xlabel="", ylabel="")
ax6.set(xlabel="", ylabel="")
ax7.set(xlabel="", ylabel="")
ax8.set(xlabel="", ylabel="")
ax9.set(xlabel="", ylabel="")

plt.show()


```
same values but with a more concise plotting style. Overall, denoising style did not impact the overall number of specific scans. once the analysis is complete with all scans, i will pick the better performing method to carry out a singular analysis 

```{python} 

df["s1.cat.gsr2"].value_counts(sort=False).reindex(cat_index, fill_value=0)
specificity_denoise= pd.concat([df["s1.cat.gsr1"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.gsr2"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.gsr3"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.wmcsf1"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.wmcsf2"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.wmcsf3"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.aCompCor1"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.aCompCor2"].value_counts(sort=False).reindex(cat_index, fill_value=0),
df["s1.cat.aCompCor3"].value_counts(sort=False).reindex(cat_index, fill_value=0)], 
axis=1)
specificity_denoise.columns=["gsr1","gsr2","gsr3","wmcsf1","wmcsf2","wmcsf3","aCompCor1","aCompCor2","aCompCor3"]

specificity_denoise = specificity_denoise/specificity_denoise.sum()

specificity_denoise_T = specificity_denoise.T
specificity_denoise_T.reset_index(level=0, inplace=True) 
specificity_denoise_T = specificity_denoise_T[["index","Specific", "Spurious", "Unspecific","No"]]

specificity_denoise_T.set_index('index').plot(kind='bar', stacked=True)

plt.show()


```

make the standard s1-s1 / s1-aca connectivity plot.   

```{python}

analysis = 'aCompCor3'

ax6 = sns.jointplot(data=df, x='s1.specific.'+analysis, y='s1.unspecific.'+analysis)

ax6.fig.suptitle('Functional connectivity specificity')
ax6.fig.subplots_adjust(top=0.9)
ax6.ax_joint.set(xlabel='Specific ROI [r]', ylabel='Unspecific ROI [r]')
ax6.ax_joint.vlines(0.1,ymin=min(df['s1.unspecific.'+analysis]),ymax=max(df['s1.unspecific.'+analysis]),linestyles='dashed', color='black')
ax6.ax_joint.vlines(-0.1, -0.1,0.1,linestyles='dashed', color='black')
ax6.ax_joint.hlines(-0.1, -0.1,0.1,linestyles='dashed', color='black')
ax6.ax_joint.hlines(0.1, -0.1,xmax=max(df['s1.specific.'+analysis]),linestyles='dashed', color='black')
ax6.ax_marg_x.axvline(x=0.1, color='black')
ax6.ax_marg_y.axhline(y=0.1, color='black')

plt.show()

```

now use nilearn to make group average maps and plot them. 
```{python}
from nilearn.image import concat_imgs, threshold_img,math_img, resample_img
from nilearn.glm.second_level import SecondLevelModel
from nilearn.plotting import plot_stat_map
import matplotlib.pyplot as plt
import pandas as pd


seed = seed_ref
analysis = 'gsr1'
p = join(data_dir, analysis, seed_img_dir)
second_level_input = df[df["rodent.ds"]=='1004']["scan_dir"].apply(lambda x: get_seed_path(p, x, seed)).reset_index(drop=True) 

design_matrix = pd.DataFrame([1] * len(second_level_input), columns=['intercept'])
        
second_level_model = SecondLevelModel(mask_img=mask_img) 
second_level_model = second_level_model.fit(second_level_input,design_matrix=design_matrix)

z_map = second_level_model.compute_contrast(output_type='z_score')

#filename_export = 'DS-'+str(i)+"_seed-"+seed
#filename_path = os.path.join(analysis_folder, 'export', output_nii, filename_export)
#z_map.to_filename(filename_path+'.nii.gz')

plot_stat_map(z_map, bg_img,title='seed: ' + seed + ', n = '+ str(len(second_level_input)), threshold=1.9, vmax=5, symmetric_cbar=True, cmap='coolwarm',  black_bg=False, cut_coords=(3,0.8,2.5))

plt.show()

output_nii = os.path.join(analysis_folder, 'export', 'group_SBA')
output_img = os.path.join(analysis_folder, 'export', 'group_SBA_img')
os.makedirs(output_nii, exist_ok=True)
os.makedirs(output_img, exist_ok=True)

# re-read all seed files in the path.
condtion = 'GSRs'
seed_list = glob.glob(
    (os.path.join(analysis_folder, 'export', 'seed', condtion))+'/*')
seed_group = ['S1bf','ACA', 'CPu','MOp']
y_stack = [0.14, 2.2, 1.6, 3.1]

for i in list(df_exclude['rat.ds'].unique()):
    for count, seed in enumerate(seed_group):

        r = re.compile(seed)
        seed_list_sub = list(filter(r.findall, seed_list))
        r = re.compile(str(i))
        seed_list_sub = list(filter(r.findall, seed_list_sub))
        r = re.compile('ses-1')
        seed_list_sub = list(filter(r.findall, seed_list_sub))
        r = re.compile("(?=(" + "|".join(map(re.escape, map(str,
               df_exclude['rat.sub'].loc[(df_exclude['rat.ds'] == i)]))) + "))")
        seed_list_sub = list(filter(r.findall, seed_list_sub))
        
        maskimg_rs = resample_img(maskimg,interpolation='nearest', target_shape=(64, 109, 64), 
                        target_affine=np.array([[0.30000001,0.,-0., -9.43999958],
                        [0.,0.30000001,-0.,-17.80999947],
                        [0.,0.,0.30000001,-6.76499987],
                        [0.,0.,0.,1.]]))
        
        for count_seedlist, seed_indiv in enumerate(seed_list_sub):
            seed_indiv_rs = resample_img(seed_indiv, target_shape=(64, 109, 64), 
                        target_affine=np.array([[0.30000001,0.,-0., -9.43999958],
                        [0.,0.30000001,-0.,-17.80999947],
                        [0.,0.,0.30000001,-6.76499987],
                        [0.,0.,0.,1.]]))
            seed_indiv_rs.to_filename(seed_indiv)
    
        second_level_input = seed_list_sub
        design_matrix = pd.DataFrame([1] * len(seed_list_sub), columns=['intercept'])
        
        second_level_model = SecondLevelModel(mask_img=maskimg_rs) 
        
        second_level_model = second_level_model.fit(second_level_input,design_matrix=design_matrix)

        z_map = second_level_model.compute_contrast(output_type='z_score')
        
        filename_export = 'DS-'+str(i)+"_seed-"+seed
        filename_path = os.path.join(analysis_folder, 'export', output_nii, filename_export)

        z_map.to_filename(filename_path+'.nii.gz')

        filename_path = os.path.join(analysis_folder, 'export', output_img, filename_export)
        plot_stat_map(z_map, bg_img, title='DS:' + str(i) +', seed: ' + seed + ', n = '+ str(len(seed_list_sub)), threshold=1.9, vmax=5, symmetric_cbar=True, cmap='coolwarm', black_bg=False,cut_coords=(0, y_stack[count], 5),output_file=filename_path+'.svg')
        #remake plot with output to jupyter notebook if S1bf seed
        if(seed == 'S1bf'):
            plot_stat_map(z_map, bg_img,title='DS:' + str(i) + ', seed: ' + seed + ', n = '+ str(len(seed_list_sub)), threshold=1.9, vmax=5, symmetric_cbar=True, cmap='coolwarm',  black_bg=False, cut_coords=(0, y_stack[count], 5))
```

